{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from pandas import json_normalize\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import date, datetime, timedelta, timezone\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "from time import sleep\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # API Camp to Camp - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:/Users/Mossuz Damien/Documents/Python Scripts/01 - C2C API/input_/merged_file.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-4118b6190bf4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;31m#Import merged results -\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"merged_file.json\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0minfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m      \u001b[0mjson_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/Users/Mossuz Damien/Documents/Python Scripts/01 - C2C API/input_/merged_file.json'"
     ]
    }
   ],
   "source": [
    "# #Export data from C2C API\n",
    "\n",
    "url = 'https://api.camptocamp.org/outings'\n",
    "france = '&a=14274'\n",
    "req_activities = {'?act=skitouring':310,'?act=rock_climbing':330,'?act=mountain_climbing':100,'?act=snow_ice_mixed':100,'?act=ice_climbing':100,'?act=hiking':100}\n",
    "\n",
    "request_num = 0\n",
    "result=[]\n",
    "\n",
    "for k,v in req_activities.items():\n",
    "    for i in range(0,v):\n",
    "        if i == 0 : \n",
    "            api_path = url + k + france\n",
    "        else:\n",
    "            api_path = url + k + france + '&offset=' + str(i*30)            \n",
    "        print(api_path)\n",
    "        r = requests.get(api_path)\n",
    "        result.append(r.json())\n",
    "        request_num += 1\n",
    "        sleep(randint(1,3))\n",
    "        \n",
    "print(\"Nombre total de reqûetes :\" + str(request_num))\n",
    "        \n",
    "path = 'C:/Users/Mossuz Damien/Documents/Python Scripts/01 - C2C API/input_/'\n",
    "\n",
    "with open(path + \"merged_file.json\", \"w\",encoding='utf-8') as outfile:\n",
    "     json.dump(result, outfile, ensure_ascii=False,indent=4)\n",
    "\n",
    "#Import merged results -        \n",
    "with open(path + \"merged_file.json\", \"r\",encoding='utf-8') as infile:\n",
    "     json_data = json.load(infile)\n",
    "\n",
    "#Check - \n",
    "print(len(json_data))\n",
    "print(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # Parse json file & data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#>>>Y-a-t-il un moyen de traiter plus rapidement les imports que je récupère?\n",
    "#>>>Créer des fonctions pour ceux que je n'arrive pas à récupérer avec json_normalize - \n",
    "\n",
    "#Step 1: flatten json files - iteratively -\n",
    "\n",
    "d = pd.DataFrame()\n",
    "\n",
    "# d= json_normalize(json_data[0]['documents'],max_level=1)\n",
    "\n",
    "for k in range(len(json_data)):\n",
    "    #Voir comment faire en partant d'un fichier JSON enregistré sur mon PC\n",
    "\n",
    "    #Normalize json\n",
    "    d=d.append(json_normalize(json_data[k]['documents'],max_level=1))\n",
    "    # d= json_normalize(json_data,max_level=1)\n",
    "\n",
    "\n",
    "    #Geometry - Voir si je peux simplifier?\n",
    "    # arr_lat=[]\n",
    "    # arr_long=[]\n",
    "    # arr_id=[]\n",
    "\n",
    "    # for k in range(len(json_data['documents'])):\n",
    "    #     arr_lat.append(json.loads(json_data['documents'][k]['geometry']['geom'])['coordinates'][0])\n",
    "    #     arr_long.append(json.loads(json_data['documents'][k]['geometry']['geom'])['coordinates'][1])\n",
    "    #     arr_id.append(json_data['documents'][k]['document_id'])\n",
    "\n",
    "    # d['latitude']=arr_lat\n",
    "    # d['longitude']=arr_long\n",
    "    # d['doc_id_test']=arr_id\n",
    "\n",
    "    #Check - OK\n",
    "    # print(np.sum(d['document_id']==d['doc_id_test']))\n",
    "\n",
    "    #Locales - OK - A adapter pour la boucle\n",
    "    # d_locales = json_normalize(json_data['documents'], record_path=['locales'], meta='document_id')\n",
    "    # d = pd.merge(left=d, right=d_locales,on='document_id')\n",
    "\n",
    "    #Areas - A d'utilité -> on ne prend pas\n",
    "    # test = json_normalize(json_data['documents'], record_path=['areas'])\n",
    "    # test\n",
    "    # test.loc[0,'locales']\n",
    "\n",
    "    # print(d.columns)\n",
    "\n",
    "# for k in d.columns:\n",
    "#     print(str(k) + ' - ' + str(d.loc[3,k]))\n",
    "\n",
    "d.head()\n",
    "# print(d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#COLUMNS Selection & order - ID, user, acitivité, date, difficulté, condition - \n",
    "cols = ['document_id','activities','date_start','date_end','condition_rating','author.name','author.user_id','labande_global_rating','ski_rating','global_rating','engagement_rating','hiking_rating','equipment_rating','rock_free_rating','ice_rating','elevation_max','height_diff_up','height_diff_difficulties']\n",
    "d=d[cols]\n",
    "\n",
    "#Reset index\n",
    "d.reset_index(inplace=True, drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning Activities - \n",
    "def binary(x,a):\n",
    "    if a in x:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "activities=['skitouring', 'rock_climbing','mountain_climbing','ice_climbing','hiking','snow_ice_mixed']\n",
    "\n",
    "for activity in activities:\n",
    "    d[activity]=d.activities.apply(lambda x: binary(x,activity))\n",
    "\n",
    "d.drop('activities', inplace=True, axis=1)\n",
    "\n",
    "#Drop_duplicates - Doublon document_id à cause du multi activités\n",
    "print('Nombre de doublons de document_id : {}'.format(np.sum(d.duplicated(subset='document_id'))))\n",
    "d.drop_duplicates(inplace=True)\n",
    "print('Nombre de doublons de document_id : {}'.format(np.sum(d.duplicated(subset='document_id'))))\n",
    "\n",
    "#Check - Date min pour chaque activité\n",
    "#>>>>>Code à revoir\n",
    "# d_date_min = d.groupby(by='activities')['date_start'].min()\n",
    "# print(d_date_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- DATA Types -\n",
    "\n",
    "# Categorical data -\n",
    "col_to_category=['condition_rating','labande_global_rating','ski_rating','global_rating','engagement_rating','hiking_rating','equipment_rating','rock_free_rating','ice_rating']\n",
    "\n",
    "for c in col_to_category:\n",
    "    d[c]=d[c].astype('category')\n",
    "    \n",
    "#>>>Category avec ordre pour toutes les cotations    \n",
    "d['global_rating'] = pd.Categorical(d['global_rating'], ['F','F+','PD-','PD','PD+','AD-','AD','AD+','D-','D','D+','TD-','TD','TD+','ED-','ED','ED+','ED4','ED5','ED7'],ordered=True)\n",
    "d['global_rating_codes'] = d['global_rating'].cat.codes\n",
    "d['rock_free_rating'] = pd.Categorical(d['rock_free_rating'], ['2','3a','3b','3c','4a','4b','4c','5a','5a+','5b','5b+','5c','5c+','6a','6a+','6b','6b+','6c','6c+','7a','7a+','7b','7b+','7c','7c+','8a','8a+'],ordered=True)\n",
    "d['rock_free_rating_codes'] = d['rock_free_rating'].cat.codes\n",
    "#X[categorical_cols] = X[categorical_cols].apply(lambda col: le.fit_transform(col))\n",
    "\n",
    "# Datetime data\n",
    "col_to_date=['date_end','date_start']\n",
    "d[col_to_date]=d[col_to_date].apply(lambda x: pd.to_datetime(x))\n",
    "\n",
    "# Float\n",
    "col_to_float =['elevation_max','height_diff_up']\n",
    "d[col_to_float]=d[col_to_float].astype(float)\n",
    "\n",
    "# Int\n",
    "col_to_int=['document_id','author.user_id']\n",
    "d[col_to_int]=d[col_to_int].astype(int)\n",
    "\n",
    "# CHECK\n",
    "print(d.info())\n",
    "d.head()\n",
    "\n",
    "d['month']=d.date_start.dt.month\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
